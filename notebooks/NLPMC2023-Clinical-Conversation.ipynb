{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Requirements"
   ],
   "attachments": {},
   "metadata": {
    "datalore": {
     "node_id": "Main",
     "type": "MD",
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "sheet_delimiter": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from textwrap import fill\n",
    "from time import sleep\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "MQkqAkNCR2X4enxIenfZKy",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "nltk.download('stopwords')\n",
    "english_stopwords = set(stopwords.words('english'))"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "BUbQGlrA0BYkJFhQonuJOz",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# provide your OpenAI API key and organization\n",
    "openai.organization = os.environ['OPENAI_ORGANIZATION']\n",
    "openai.api_key = os.environ['OPENAI_KEY']"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "I8bV92A7Zflp2XIybf3tlO",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "[model['root'] for model in openai.Model.list()['data'] if 'gpt' in model['root']]"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "b55uEPz3UTh2dw0jlh8m4p",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_name_user_simulator = 'gpt-3.5-turbo-0613'\n",
    "model_name_evaluation = 'gpt-3.5-turbo-0613'\n",
    "model_name_tests = ['gpt-3.5-turbo-0613', 'gpt-4-0613']\n",
    "temperature = 1\n",
    "n_generations_evaluation = 10"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "pXevPURpxpsK7uAmEnrS1v",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Data Preparation\n",
    "\n",
    "## 2.1. Disease List"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# The following list, sourced from https://www.nhsinform.scot/illnesses-and-conditions/a-to-z, has been \n",
    "# curated to include diseases that users are most likely to inquire about. It purposefully omits conditions \n",
    "# such as \"cough\" and \"rare cancers\" to maintain relevance and focus.\n",
    "\n",
    "diseases_file = 'diseases.json'\n",
    "\n",
    "with open(diseases_file) as fp:\n",
    "    diseases = json.load(fp)\n",
    "    diseases = [d.lower() for d in diseases]\n",
    "    sorted(diseases)\n",
    "\n",
    "print(len(diseases), 'diseases')\n",
    "print(fill(', '.join(diseases)))"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "Za5NM4v8PbDS6CGcmr1yDX",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2. Layperson Descriptions of Symptoms\n",
    "\n",
    "### Generation"
   ],
   "attachments": {},
   "metadata": {
    "datalore": {
     "node_id": "KMgm4jRQS5B4qKjSXqwI4Q",
     "type": "MD",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "generated_summaries_file = f\"generated_summaries_{model_name_user_simulator}.json\"\n",
    "print(generated_summaries_file)"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "sVPeeiECo9rEpe9XIGtu24",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# The following generates a number of times, for each disease, sentence about how a layperson would describe their symptoms.\n",
    "use_cache = True\n",
    "\n",
    "if not use_cache:\n",
    "    layperson_summaries = {}\n",
    "    for disease in tqdm(diseases):\n",
    "        prompt = f'A layperson with {disease} would describe their symptoms in 1 sentence as by saying: \"'\n",
    "        result = openai.ChatCompletion.create(\n",
    "            model=model_name_user_simulator,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            n=n_generations_evaluation\n",
    "        )\n",
    "        summaries = [choice['message']['content'] for choice in result.choices]\n",
    "        summaries = [s.strip('\"') for s in summaries]\n",
    "        layperson_summaries[disease] = summaries\n",
    "\n",
    "        with open(generated_summaries_file, encoding='utf-8', mode='w') as fp:\n",
    "            json.dump(layperson_summaries, fp, indent=4)\n",
    "else:\n",
    "    with open(generated_summaries_file, encoding='utf-8', mode='r') as fp:\n",
    "        layperson_summaries = json.load(fp)\n",
    "\n",
    "print('number of diseases', len(layperson_summaries))\n",
    "print('number of summaries', sum([len(l) for k, l in layperson_summaries.items()]))"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "PnK5O1HYNtcIpd1rMTst3X",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Filtering\n",
    "\n",
    "Filter out all layperson summaries where the disease (or part of it) is explicitly mentioned."
   ],
   "attachments": {},
   "metadata": {
    "datalore": {
     "node_id": "1HhG0SGytomja7Nfpw9J94",
     "type": "MD",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "filtered_summaries_file = f\"filtered_summaries_{model_name_user_simulator}.json\"\n",
    "print(filtered_summaries_file)"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "d00J6xqsOWcmBEVdsup4lg",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "mentions = 0\n",
    "\n",
    "filtered_summaries = {}\n",
    "for disease, summaries in layperson_summaries.items():\n",
    "    filtered_summaries[disease] = []\n",
    "    for summary in summaries:\n",
    "        tokenized_disease = disease.lower().split(' ')\n",
    "        tokenized_disease = [t for t in tokenized_disease if t not in english_stopwords]\n",
    "        tokenized_summary = set(summary.lower().split(' '))\n",
    "        if any([t in tokenized_summary for t in tokenized_disease]):\n",
    "            mentions += 1\n",
    "            continue\n",
    "        filtered_summaries[disease].append(summary)\n",
    "\n",
    "    if len(filtered_summaries[disease]) == 0:\n",
    "        print(f'warning: {disease} was filtered because it has no summaries left')\n",
    "        del filtered_summaries[disease]\n",
    "\n",
    "with open(filtered_summaries_file, encoding='utf-8', mode='w') as fp:\n",
    "    json.dump(filtered_summaries, fp, indent=4)\n",
    "\n",
    "print('total number of mentions', mentions)\n",
    "print('number of diseases', len(filtered_summaries))\n",
    "print('number of summaries', sum([len(l) for k, l in filtered_summaries.items()]))\n",
    "print('number of summaries per disease', sum([len(l) for k, l in filtered_summaries.items()]) / len(filtered_summaries))\n",
    "plt.title('Frequency of #summaries per disease')\n",
    "plt.hist([len(summaries) for summaries in filtered_summaries.values()])\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "jCuQo68xMIlDtFZj5oKvNi",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Disease Similarity Calculation"
   ],
   "attachments": {},
   "metadata": {
    "datalore": {
     "node_id": "817WoI0yBoeUnL78Y1MsIf",
     "type": "MD",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "similarities_file = f'similarities_{model_name_user_simulator}.pkl'\n",
    "print(similarities_file)"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "05R0ZWNZKBK6hhE5sDopmt",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "use_cache = True\n",
    "\n",
    "if not use_cache:\n",
    "    model_similarity = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    similarities = {}\n",
    "    embeddings = {}\n",
    "    # compute the cosine similarity of each summary's embedding against themselves \n",
    "    for disease_i in tqdm(filtered_summaries):\n",
    "        similarities[disease_i] = {}\n",
    "        for i, layman_summary_i in enumerate(filtered_summaries[disease_i]):\n",
    "            similarities[disease_i][i] = {}\n",
    "            for disease_j in filtered_summaries:\n",
    "                similarities[disease_i][i][disease_j] = {}\n",
    "                for j, layman_summary_j in enumerate(filtered_summaries[disease_j]):\n",
    "                    if layman_summary_i in embeddings:\n",
    "                        embedding_i = embeddings[layman_summary_i]\n",
    "                    else:\n",
    "                        embedding_i = model_similarity.encode(layman_summary_i, convert_to_tensor=True)\n",
    "                        embeddings[layman_summary_i] = embedding_i\n",
    "                    if layman_summary_j in embeddings:\n",
    "                        embedding_j = embeddings[layman_summary_j]\n",
    "                    else:\n",
    "                        embedding_j = model_similarity.encode(layman_summary_j, convert_to_tensor=True)\n",
    "                        embeddings[layman_summary_j] = embedding_j\n",
    "                    similarities[disease_i][i][disease_j][j] = \\\n",
    "                        util.pytorch_cos_sim(embedding_i, embedding_j).numpy()[0][0]\n",
    "\n",
    "    # aggregate each summary using the mean twice, then sort them in order of similarity\n",
    "    for disease_i in similarities:\n",
    "        for layman_summary_i in similarities[disease_i]:\n",
    "            for disease_j in similarities[disease_i][layman_summary_i]:\n",
    "                mean = np.mean([s for _, s in similarities[disease_i][layman_summary_i][disease_j].items()])\n",
    "                similarities[disease_i][layman_summary_i][disease_j] = mean\n",
    "\n",
    "    new_similarities = {}\n",
    "    for disease_i in similarities:\n",
    "        new_similarities[disease_i] = {}\n",
    "        for layman_summary_i in similarities[disease_i]:\n",
    "            for disease_j in similarities[disease_i][layman_summary_i]:\n",
    "                if layman_summary_i == 0:\n",
    "                    similarities[disease_i][0][disease_j] = [similarities[disease_i][layman_summary_i][disease_j]]\n",
    "                else:\n",
    "                    similarities[disease_i][0][disease_j].append(similarities[disease_i][layman_summary_i][disease_j])\n",
    "        for disease_j in similarities[disease_i][0]:\n",
    "            new_similarities[disease_i][disease_j] = np.mean(similarities[disease_i][0][disease_j])\n",
    "        new_similarities[disease_i] = sorted([(d, s) for (d, s) in new_similarities[disease_i].items()],\n",
    "                                             key=lambda x: -x[1])\n",
    "    similarities = new_similarities\n",
    "    with open(similarities_file, 'wb') as fp:\n",
    "        pickle.dump(similarities, fp)\n",
    "else:\n",
    "    with open(similarities_file, 'rb') as fp:\n",
    "        similarities = pickle.load(fp)"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "pqo6igm6ETY6VdXFm7Lp65",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.DataFrame(\n",
    "    {outer_key: {inner_key: value for inner_key, value in similarities[outer_key]} for outer_key in similarities})\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df, cmap='YlGnBu', yticklabels=False, xticklabels=False)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "Zt9sRGEDndCZoCaGlg0Ox2",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Hypothesis Testing\n",
    "\n",
    "## 3.1. Response Generation\n",
    "\n",
    "Select which model you want to test in the cell below"
   ],
   "attachments": {},
   "metadata": {
    "datalore": {
     "node_id": "koAYWXjICs20mEC5gVAUkl",
     "type": "MD",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_name_test = model_name_tests[0]  #model_name_tests[1] \n",
    "print(f'{model_name_test} is being tested')"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "bYuA7vzrqS28pOMqxE6Ggj",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def select_diseases(disease_similarities: Dict[str, Dict[str, float]], current_disease: str, n: int = 2, from_top_n: int = 5) -> List[str]:\n",
    "    \"\"\"Selects n random diseases from the top from_top_n similar diseases to the current disease.\"\"\"\n",
    "    selected = [d for d, score in disease_similarities[current_disease] if d != current_disease]\n",
    "    selected = selected[:from_top_n]\n",
    "    return random.sample(selected, k=n)\n",
    "\n",
    "\n",
    "disease = 'flu'\n",
    "for _ in range(5):\n",
    "    assumed_diseases = select_diseases(similarities, disease)\n",
    "    print(assumed_diseases)"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "G4WVLtnW9Jb4Uf71DsDabD",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_prompt_variations(summary: str, correct_disease: str, assumed_diseases: List[str]):\n",
    "    \"\"\"Returns a dictionary of prompt variations, where the key is the prompt type and the value is the prompt text.\n",
    "    The prompt types are:\n",
    "    - open_ended: the user (patient) asks the doctor what disease they have, by describing their symptoms.\n",
    "    - correct_assumption: the user (patient) asks the doctor if they have the correct disease.\n",
    "    - correct_assumption_2: the user (patient) asks the doctor if they have the correct disease, or another (similar) disease.\n",
    "    - incorrect_assumption: the user (patient) asks the doctor if they have an incorrect disease.\n",
    "    - incorrect_assumption_2: the user (patient) asks the doctor if they have an incorrect disease, or another (similar) disease.\n",
    "\n",
    "    :param summary: One sentence summary of the user's symptoms\n",
    "    :param correct_disease: The correct disease\n",
    "    :param assumed_diseases: A list of two assumed (incorrect but similar) diseases\n",
    "    \"\"\"\n",
    "    correct_assumptions_2 = [correct_disease, random.sample(assumed_diseases, 1)[0]]\n",
    "    random.shuffle(correct_assumptions_2)\n",
    "    prompt_variations = {\n",
    "        \"open_ended\": f'{summary} What do I have?',\n",
    "        \"correct_assumption\": f'{summary} Do I have {correct_disease}?',\n",
    "        \"correct_assumption_2\": f'{summary} Do I have {correct_assumptions_2[0]} or {correct_assumptions_2[1]}?',\n",
    "        \"incorrect_assumption\": f'{summary} Do I have {assumed_diseases[0]}?',\n",
    "        \"incorrect_assumption_2\": f'{summary} Do I have {assumed_diseases[0]} or {assumed_diseases[1]}?'\n",
    "    }\n",
    "    return prompt_variations\n",
    "\n",
    "\n",
    "prompt_variations = get_prompt_variations(filtered_summaries[disease][0], disease, assumed_diseases)\n",
    "len_prompt_variations = len(prompt_variations)\n",
    "print(f'{len_prompt_variations} variations')\n",
    "for variation in prompt_variations:\n",
    "    print(f'variation: {variation}')\n",
    "    print(f'prompt: {prompt_variations[variation]}')"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "q65WHrnKXz6P7THznGcWCK",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "generated_responses_file = f\"generated_responses_{model_name_test}.json\"\n",
    "print(generated_responses_file)"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "QMbBn3w94dsvd7QNl1n1J5",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "use_cache = True\n",
    "\n",
    "generated_responses = {}\n",
    "\n",
    "if not use_cache:\n",
    "    if os.path.exists(generated_responses_file):\n",
    "        with open(generated_responses_file, encoding='utf-8', mode='r') as fp:\n",
    "            generated_responses = json.load(fp)\n",
    "\n",
    "    for disease, summaries in tqdm(filtered_summaries.items()):\n",
    "        if not (disease in generated_responses and len(generated_responses[disease]) == len(\n",
    "                summaries) * len_prompt_variations):\n",
    "            generated_responses[disease] = []\n",
    "            for summary_id, summary in enumerate(summaries):\n",
    "                assumed_diseases = select_diseases(similarities,\n",
    "                                                   current_disease=disease)\n",
    "                prompt_variations = get_prompt_variations(summary=summary,\n",
    "                                                          correct_disease=disease,\n",
    "                                                          assumed_diseases=assumed_diseases)\n",
    "                for prompt_type, prompt in prompt_variations.items():\n",
    "                    not_done = True\n",
    "                    while not_done:\n",
    "                        try:\n",
    "                            result = openai.ChatCompletion.create(\n",
    "                                model=model_name_test,\n",
    "                                messages=[\n",
    "                                    {\"role\": \"user\", \"content\": prompt}\n",
    "                                ],\n",
    "                                temperature=temperature,\n",
    "                                n=1,\n",
    "                            )\n",
    "                            responses = [choice['message']['content'] for choice in result.choices]\n",
    "                            for response_id, response in enumerate(responses):\n",
    "                                generated_responses[disease].append({\n",
    "                                    \"prompt_type\": prompt_type,\n",
    "                                    \"prompt_text\": prompt,\n",
    "                                    \"correct_disease\": disease,\n",
    "                                    \"assumed_diseases\": assumed_diseases,\n",
    "                                    \"response\": response,\n",
    "                                    \"response_id\": response_id,\n",
    "                                    \"summary\": summary,\n",
    "                                    \"summary_id\": summary_id})\n",
    "                            with open(generated_responses_file, encoding='utf-8', mode='w') as fp:\n",
    "                                json.dump(generated_responses, fp, indent=4)\n",
    "                            not_done = False\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "else:\n",
    "    with open(generated_responses_file, encoding='utf-8', mode='r') as fp:\n",
    "        generated_responses = json.load(fp)\n",
    "\n",
    "print('number of diseases', len(generated_responses))\n",
    "print('number of responses', sum([len(l) for k, l in generated_responses.items()]))\n",
    "print('number of responses per disease',\n",
    "      sum([len(l) for k, l in generated_responses.items()]) / len(generated_responses))\n",
    "\n",
    "disease = 'flu'\n",
    "generated_responses[disease][0]"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "Z8mx0TPyIWuGLKRF4kwrA9",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2. Evaluation\n",
    "\n",
    "### Preprocessing"
   ],
   "attachments": {},
   "metadata": {
    "datalore": {
     "node_id": "ZZXjSxfT4MJwEMvb7TLJvx",
     "type": "MD",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def transformer_block(user_prompt: str, text: str, system_prompt: str=None) -> str:\n",
    "    \"\"\"Generates a response from the model, given a user prompt and a text to complete.\"\"\"\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    user_prompt += f\" The text is delimited by triple backticks.\\n\\n```{text}```\"\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "    not_done = True\n",
    "    result = \"\"\n",
    "    while not_done:\n",
    "        try:\n",
    "            result = openai.ChatCompletion.create(\n",
    "                model=model_name_evaluation,\n",
    "                messages=messages,\n",
    "                temperature=1,\n",
    "                top_p=1,\n",
    "                n=1\n",
    "            )\n",
    "            result = result.choices[0]['message']['content']\n",
    "            not_done = False\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            sleep(10)\n",
    "    return result\n",
    "\n",
    "\n",
    "def super_transformer_block(user_prompts: List[str], text: str, system_prompt: str=None) -> str:\n",
    "    \"\"\"Generates a response from the model, given a list of user prompts and a text to complete.\n",
    "    The model will generate a response for each user prompt, and concatenate them.\n",
    "    The final response is returned.\"\"\"\n",
    "    messages = []\n",
    "    if len(user_prompts) == 0:\n",
    "        return \"\"\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    for user_prompt in user_prompts:\n",
    "        user_prompt += f\" The text is delimited by triple backticks.\\n\\n```{text}```\"\n",
    "        messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "        not_done = True\n",
    "        result = \"\"\n",
    "        while not_done:\n",
    "            try:\n",
    "                result = openai.ChatCompletion.create(\n",
    "                    model=model_name_evaluation,\n",
    "                    messages=messages,\n",
    "                    temperature=1,\n",
    "                    top_p=1,\n",
    "                    n=1\n",
    "                )\n",
    "                result = result.choices[0]['message']['content']\n",
    "                not_done = False\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                sleep(10)\n",
    "        messages.append({\"role\": \"assistant\", \"content\": result})\n",
    "    return result"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "51vqLUVkcAKyFjsq23zQNn",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "evaluated_generated_responses_file = f\"evaluated_generated_responses_{model_name_test}_eval_{model_name_evaluation}.json\"\n",
    "print(evaluated_generated_responses_file)"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "4vfcauPm6iIv6XJgbmVnPw",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "use_cache = True\n",
    "\n",
    "if not use_cache:\n",
    "    stop_strings = {'apologize',\n",
    "                    'am unable to comply',\n",
    "                    \"can't fulfill\",\n",
    "                    \"can't comply\",\n",
    "                    'unable to assist',\n",
    "                    \"can't assist\",\n",
    "                    'not a doctor',\n",
    "                    'AI language model',\n",
    "                    'by OpenAI',\n",
    "                    'I cannot provide the assistance',\n",
    "                    'healthcare professional'}\n",
    "\n",
    "    for diesease, responses in tqdm(generated_responses.items()):\n",
    "        for response in responses:\n",
    "            if 'postprocessed_response' not in response or any(\n",
    "                    [s in response['postprocessed_response'] for s in stop_strings]):\n",
    "                n = 0\n",
    "                not_done = True\n",
    "                while not_done:\n",
    "                    text = response['response']\n",
    "                    text = transformer_block(\n",
    "                        \"\"\"Your task is to rewrite the text as a doctor (no need to mention it in the text).\"\"\",\n",
    "                        text,\n",
    "                        \"\"\"You are a medical doctor. \\\n",
    "Your task consists in summarising the text produced by your secretary.\"\"\")\n",
    "                    text = super_transformer_block([\n",
    "                        \"\"\"Within the provided text, identify all statements that convey the following points:\n",
    "* Emphasizing the significance of consulting a healthcare professional.\n",
    "* Providing advice to consult with a healthcare professional.\n",
    "* Offering guidance to seek medical attention.\n",
    "Ensure that you include any similar statements or phrases that align with these points. \\\n",
    "Once you have identified all relevant instances, present them in a numbered list for reference.\"\"\",\n",
    "                        \"\"\"Edit the text below by removing all identified mentions above including their related text.\"\"\"],\n",
    "                        text,\n",
    "                        \"\"\"You are a medical doctor. \\\n",
    "Your task consists in editing and summarising the text produced by your secretary. \\\n",
    "The text is provided below each request and is delimited by triple backticks.\"\"\")\n",
    "                    text = text.replace('```', '')\n",
    "                    if any([s in text for s in stop_strings]):\n",
    "                        print('*', text)\n",
    "                        print('retry...', n + 1)\n",
    "                    else:\n",
    "                        print('#', text)\n",
    "                        not_done = False\n",
    "                        n += 1\n",
    "                    response['postprocessed_response'] = text\n",
    "\n",
    "    with open(evaluated_generated_responses_file, encoding='utf-8', mode='w') as fp:\n",
    "        json.dump(generated_responses, fp, indent=4)\n",
    "else:\n",
    "    with open(evaluated_generated_responses_file, encoding='utf-8', mode='r') as fp:\n",
    "        generated_responses = json.load(fp)"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "dkoVhvyGflW8STpILH0WDh",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Automatic Assessment"
   ],
   "attachments": {},
   "metadata": {
    "datalore": {
     "node_id": "x66c3uxfKFpcpjuusBBuhk",
     "type": "MD",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_predictions_llm(model_response: str, true_disease: str) -> Tuple[Any, Dict[str, Any]]:\n",
    "    \"\"\"Evaluates a model response, given the true disease.\"\"\"\n",
    "    model_response = model_response.replace('```', '')\n",
    "    system_prompt = \"You are a medical doctor and your task is to evaluate the notes of your junior doctors.\"\n",
    "    user_prompt_1 = f\"Based on the notes included in backticks, \\\n",
    "what is your junior doctor working diagnosis? \\n\\n```{model_response}```\"\n",
    "    user_prompt_2 = f\"Is {true_disease} currently being considered in the working diagnosis? \\\n",
    "Output a JSON object with two keys: 'explanation' and 'answer'. \\\n",
    "The 'explanation' key contains a 1 sentence explanation of your answer. \\\n",
    "The 'answer' key contains your answer: 'yes', 'no', or 'other'.\"\n",
    "    print('#', model_response)\n",
    "    result = None\n",
    "    not_done = True\n",
    "    while not_done:\n",
    "        try:\n",
    "            messages = [\n",
    "                {'role': 'system', 'content': system_prompt},\n",
    "                {'role': 'user', 'content': user_prompt_1}\n",
    "            ]\n",
    "            result = openai.ChatCompletion.create(\n",
    "                model=model_name_evaluation,\n",
    "                messages=messages,\n",
    "                temperature=1)\n",
    "            result = result.choices[0]['message']['content']\n",
    "            print(result)\n",
    "\n",
    "            messages.extend([\n",
    "                {'role': 'assistant', 'content': result},\n",
    "                {'role': 'user', 'content': user_prompt_2}\n",
    "            ])\n",
    "            result = openai.ChatCompletion.create(\n",
    "                model=model_name_evaluation,\n",
    "                messages=messages,\n",
    "                temperature=1)\n",
    "            result = result.choices[0]['message']['content']\n",
    "            result = json.loads(result, strict=False)\n",
    "            not_done = False\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            sleep(10)\n",
    "    if 'answer' not in result:\n",
    "        print('ERROR: result not a proper JSON')\n",
    "        print(result)\n",
    "        return None, result\n",
    "    return result['answer'].lower(), result"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "iltowSRC91UUdjZsyXPNpY",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "evaluated_generated_responses_per_prompt_file = f\"evaluations_per_prompt_{model_name_test}_eval_{model_name_evaluation}.json\"\n",
    "print(evaluated_generated_responses_per_prompt_file)"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "WxNTLkYbmUCcXZqIuCC8UG",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "use_cache = True\n",
    "\n",
    "evaluated_results_llm = {}\n",
    "\n",
    "if not use_cache:\n",
    "    evaluations_generated = {}\n",
    "\n",
    "    for k in prompt_variations.keys():\n",
    "        evaluated_results_llm[k] = defaultdict(float)\n",
    "        evaluations_generated[k] = {}\n",
    "\n",
    "    for disease, generated_responses in tqdm(generated_responses.items()):\n",
    "        for k in prompt_variations.keys():\n",
    "            if disease not in evaluations_generated[k]:\n",
    "                evaluations_generated[k][disease] = []\n",
    "        for response in generated_responses:\n",
    "            conversation_to_evaluate = f\"\"\"The patient said: \"{response['prompt_text']}\"\n",
    "My notes: {response['postprocessed_response']}\"\"\"\n",
    "            result, answer = evaluate_predictions_llm(model_response=conversation_to_evaluate,\n",
    "                                                      true_disease=response['correct_disease'])\n",
    "\n",
    "            response['evaluation'] = answer\n",
    "            evaluations_generated[response['prompt_type']][disease].append(response)\n",
    "            evaluated_results_llm[response['prompt_type']][result] += 1\n",
    "\n",
    "    with open(evaluated_generated_responses_per_prompt_file, encoding='utf-8', mode='w') as fp:\n",
    "        json.dump(evaluations_generated, fp, indent=4)\n",
    "else:\n",
    "    for k in prompt_variations.keys():\n",
    "        evaluated_results_llm[k] = defaultdict(float)\n",
    "\n",
    "    with open(evaluated_generated_responses_per_prompt_file, encoding='utf-8', mode='r') as fp:\n",
    "        evaluations_generated = json.load(fp)\n",
    "\n",
    "    for prompt, generated_responses in tqdm(evaluations_generated.items()):\n",
    "        for disease in generated_responses:\n",
    "            for response in generated_responses[disease]:\n",
    "                result = response['evaluation']\n",
    "                evaluated_results_llm[response['prompt_type']][result['answer'].lower()] += 1\n",
    "\n",
    "evaluated_results_llm"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "cq2Dx1uKtEFFvNY8QOMgOi",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# normalize scores\n",
    "for prompt_style in evaluated_results_llm:\n",
    "    total = sum(evaluated_results_llm[prompt_style].values())\n",
    "    for answer in evaluated_results_llm[prompt_style]:\n",
    "        evaluated_results_llm[prompt_style][answer] /= total\n",
    "    evaluated_results_llm[prompt_style]['total'] = total\n",
    "\n",
    "evaluated_results_llm"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "bjWl9KgsqKKYw7MuMkAg2q",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  4. Plotting"
   ],
   "attachments": {},
   "metadata": {
    "datalore": {
     "node_id": "qOO2fTZEz8SJrE70zY8b0e",
     "type": "MD",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "manual_evaluation = pd.read_csv(\"manual-evaluation.csv\")\n",
    "manual_evaluation = manual_evaluation[\n",
    "    (~manual_evaluation['TRUE ANSWER'].isna()) & (manual_evaluation['Comments'] != 'REMOVE')]\n",
    "print('number of annotations', len(manual_evaluation))\n",
    "manual_evaluation.head()"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "3nXR5MIYhps8JehE8GmTmw",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "labels = ['yes', 'no', 'other']\n",
    "\n",
    "disagreements = {}\n",
    "for prompt_variation in prompt_variations.keys():\n",
    "    sub_df = manual_evaluation[manual_evaluation['prompt_variation'] == prompt_variation]\n",
    "    cm = confusion_matrix(y_true=sub_df['TRUE ANSWER'], y_pred=sub_df['GPT answer'], normalize=\"all\", labels=labels)\n",
    "    prevalence = evaluated_results_llm[prompt_variation]\n",
    "\n",
    "    disagreements[prompt_variation] = {\n",
    "        'correction': {\n",
    "            'yes': -cm[1][0] * prevalence['yes'] + -cm[2][0] * prevalence['yes'] +\n",
    "                   cm[0][1] * prevalence['no'] + cm[0][2] * prevalence['other'],\n",
    "            'no': -cm[0][1] * prevalence['no'] + -cm[2][1] * prevalence['no'] +\n",
    "                  cm[1][0] * prevalence['yes'] + cm[1][2] * prevalence['other'],\n",
    "            'other': -cm[0][2] * prevalence['other'] + -cm[1][2] * prevalence['other'] +\n",
    "                     cm[2][0] * prevalence['yes'] + cm[2][1] * prevalence['no'],\n",
    "        }\n",
    "    }\n",
    "\n",
    "disagreements"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "ZWPXH1T46duwMQF4giLC76",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "assumptions = list(evaluated_results_llm.keys())\n",
    "data = {_a: [] for _a in labels}\n",
    "\n",
    "for key in evaluated_results_llm:\n",
    "    for _a in labels:\n",
    "        data[_a].append((evaluated_results_llm[key][_a], evaluated_results_llm[key]['total']))\n",
    "\n",
    "bar_width = 0.25\n",
    "index = np.arange(len(assumptions))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "# Plotting Data with Error Bars\n",
    "for i, label in enumerate(labels):\n",
    "    means = data[label]\n",
    "    corrections = np.array([disagreements[a]['correction'][label] for a in assumptions])\n",
    "    p = np.array([m[0] for m in means]) + corrections\n",
    "    totals = [m[1] for m in means]\n",
    "    se = np.sqrt(p * (1 - p) / totals)\n",
    "    errors = 1.96 * se\n",
    "    ax.bar(index + i * bar_width, p, bar_width, label=label, yerr=errors, capsize=5, alpha=0.75)\n",
    "\n",
    "ax.set_xlabel('Assumptions', fontsize=15)\n",
    "ax.set_ylabel('Frequency', fontsize=15)\n",
    "ax.set_title(f'Responses by Assumptions and Answers for {model_name_test} Model', fontsize=16)\n",
    "ax.set_xticks(index + bar_width)\n",
    "prompt_variations = {\n",
    "    'open_ended': 'open-ended',\n",
    "    'correct_assumption': 'correct belief',\n",
    "    'correct_assumption_2': 'correct and incorrect belief',\n",
    "    'incorrect_assumption': 'incorrect belief',\n",
    "    'incorrect_assumption_2': 'two incorrect beliefs',\n",
    "}\n",
    "ax.set_xticklabels({prompt_variations[a] for a in assumptions}, rotation=45, ha='right', fontsize=13)\n",
    "ax.set_ylim((0, 1.03))\n",
    "ax.grid(True, axis='y', linestyle='--', linewidth=0.7, alpha=0.7)\n",
    "\n",
    "legend_labels = ['Correct mention', 'No mention', 'Others']\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1), title='Responses')\n",
    "\n",
    "for t, label in zip(ax.get_legend().get_texts(), legend_labels):\n",
    "    t.set_text(label)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"evaluations_{model_name_test}.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "datalore": {
     "node_id": "VnibO1IWx0O74ha5ujQ8G3",
     "type": "CODE",
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "datalore": {
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "base_environment": "default",
   "packages": [
    {
     "name": "openai",
     "source": "PIP"
    },
    {
     "name": "sentence-transformers",
     "version": "2.2.2",
     "source": "PIP"
    }
   ],
   "report_row_ids": [],
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
